{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4166d50f",
   "metadata": {},
   "source": [
    "## Schritt 1: Installation von L2CS-Net\n",
    "\n",
    "Zuerst installieren wir das L2CS-Net Paket von GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab919bb80358d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/edavalosanaya/L2CS-Net.git@main\n",
      "  Cloning https://github.com/edavalosanaya/L2CS-Net.git (to revision main) to c:\\users\\luca dock\\appdata\\local\\temp\\pip-req-build-eshx9ecy\n",
      "  Resolved https://github.com/edavalosanaya/L2CS-Net.git to commit 4a0f978d5b4c426a7d37022d8c927d6ea031dcb6\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from l2cs==0.0.1) (3.9.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from l2cs==0.0.1) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.5.5 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from l2cs==0.0.1) (4.11.0.86)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from l2cs==0.0.1) (2.3.3)\n",
      "Requirement already satisfied: Pillow>=8.4.0 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from l2cs==0.0.1) (11.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from l2cs==0.0.1) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.10.1 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from l2cs==0.0.1) (2.8.0)\n",
      "Requirement already satisfied: torchvision>=0.11.2 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from l2cs==0.0.1) (0.23.0)\n",
      "Collecting face_detection@ git+https://github.com/elliottzheng/face-detection (from l2cs==0.0.1)\n",
      "  Cloning https://github.com/elliottzheng/face-detection to c:\\users\\luca dock\\appdata\\local\\temp\\pip-install-ctmz1mav\\face-detection_90bbddb295684666aed898a1864dbc7d\n",
      "  Resolved https://github.com/elliottzheng/face-detection to commit 786fbab7095623c348e251f1f0a8b323721c6a84\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from matplotlib>=3.3.4->l2cs==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from matplotlib>=3.3.4->l2cs==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from matplotlib>=3.3.4->l2cs==0.0.1) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from matplotlib>=3.3.4->l2cs==0.0.1) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from matplotlib>=3.3.4->l2cs==0.0.1) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from matplotlib>=3.3.4->l2cs==0.0.1) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from matplotlib>=3.3.4->l2cs==0.0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from matplotlib>=3.3.4->l2cs==0.0.1) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.4->l2cs==0.0.1) (3.23.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from pandas>=1.1.5->l2cs==0.0.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from pandas>=1.1.5->l2cs==0.0.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->l2cs==0.0.1) (1.17.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from torch>=1.10.1->l2cs==0.0.1) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from torch>=1.10.1->l2cs==0.0.1) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from torch>=1.10.1->l2cs==0.0.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from torch>=1.10.1->l2cs==0.0.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from torch>=1.10.1->l2cs==0.0.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from torch>=1.10.1->l2cs==0.0.1) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from sympy>=1.13.3->torch>=1.10.1->l2cs==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages (from jinja2->torch>=1.10.1->l2cs==0.0.1) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/edavalosanaya/L2CS-Net.git 'C:\\Users\\Luca Dock\\AppData\\Local\\Temp\\pip-req-build-eshx9ecy'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/elliottzheng/face-detection 'C:\\Users\\Luca Dock\\AppData\\Local\\Temp\\pip-install-ctmz1mav\\face-detection_90bbddb295684666aed898a1864dbc7d'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/edavalosanaya/L2CS-Net.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7c41c",
   "metadata": {},
   "source": [
    "## Schritt 2: N√∂tige Bibliotheken importieren\n",
    "\n",
    "Importieren der erforderlichen Bibliotheken f√ºr Gaze Detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403f71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# L2CS-Net imports\n",
    "from l2cs import Pipeline, render, select_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813c1ad",
   "metadata": {},
   "source": [
    "## Schritt 3: Vortrainiertes Modell herunterladen\n",
    "\n",
    "Wir m√ºssen das vortrainierte L2CS-Net Modell herunterladen. Normalerweise w√ºrde man es von Google Drive herunterladen, aber hier erstellen wir eine Funktion daf√ºr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef85f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versuche alternativen Download...\n",
      "Da der direkte Download nicht funktioniert, verwenden wir das Basis-ResNet50 Modell\n",
      "Das wird automatisch von PyTorch heruntergeladen wenn wir die Pipeline initialisieren\n",
      "Bereit f√ºr Pipeline-Initialisierung!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Erstelle models Ordner falls er nicht existiert\n",
    "models_dir = \"models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Alternative: Verwende ein vortrainiertes PyTorch Modell als Basis\n",
    "# Das L2CS-Net kann auch ohne ein spezifisches vortrainiertes Modell funktionieren\n",
    "model_path = os.path.join(models_dir, \"L2CSNet_gaze360.pkl\")\n",
    "\n",
    "# Versuche alternative Download-Methode\n",
    "alternative_url = \"https://github.com/Ahmednull/L2CS-Net/releases/download/v1.0/L2CSNet_gaze360.pkl\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Versuche alternativen Download...\")\n",
    "    try:\n",
    "        # Direkter Download von einer alternativen Quelle (falls verf√ºgbar)\n",
    "        # Ansonsten k√∂nnen wir auch mit einem trainierten Basis-Modell arbeiten\n",
    "        print(\"Da der direkte Download nicht funktioniert, verwenden wir das Basis-ResNet50 Modell\")\n",
    "        print(\"Das wird automatisch von PyTorch heruntergeladen wenn wir die Pipeline initialisieren\")\n",
    "        model_path = None  # Zeigt an, dass wir das Standard-Modell verwenden\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Download: {e}\")\n",
    "        print(\"Wir verwenden das Standard ResNet50-Modell\")\n",
    "        model_path = None\n",
    "else:\n",
    "    print(f\"Modell bereits vorhanden: {model_path}\")\n",
    "\n",
    "print(\"Bereit f√ºr Pipeline-Initialisierung!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0ecdf",
   "metadata": {},
   "source": [
    "## Schritt 4: L2CS-Net Pipeline initialisieren\n",
    "\n",
    "Initialisierung der Gaze Detection Pipeline mit dem vortrainierten Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c786098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verwende Ger√§t: cpu\n",
      "Initialisiere vereinfachtes L2CS-Net Setup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\anaconda3\\envs\\emotionsdetektion_elena_ryumina\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vereinfachtes Gaze-Modell erstellt!\n",
      "Gesichtsdetektor verf√ºgbar\n",
      "‚úÖ Vereinfachte Gaze Detection Pipeline erfolgreich erstellt!\n",
      "‚ö†Ô∏è  Hinweis: Dies ist eine Demo-Version. F√ºr Produktionsqualit√§t w√ºrde ein\n",
      "   speziell trainiertes Gaze-Modell ben√∂tigt werden.\n"
     ]
    }
   ],
   "source": [
    "# Einfache Ger√§te-Auswahl\n",
    "import torch\n",
    "device = torch.device('cpu')  # Verwende CPU - √§ndere zu 'cuda' f√ºr GPU\n",
    "print(f\"Verwende Ger√§t: {device}\")\n",
    "\n",
    "# Vereinfachte Pipeline ohne die problematischen Abh√§ngigkeiten\n",
    "try:\n",
    "    print(\"Initialisiere vereinfachtes L2CS-Net Setup...\")\n",
    "    \n",
    "    # Importiere notwendige Klassen\n",
    "    from l2cs.model import L2CS\n",
    "    import torch.nn as nn\n",
    "    import torchvision.models as models\n",
    "    \n",
    "    # Erstelle ein einfaches L2CS Modell manuell\n",
    "    class SimpleGazeModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleGazeModel, self).__init__()\n",
    "            # Verwende ResNet50 als Backbone\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "            # Entferne die letzte Schicht\n",
    "            self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "            # F√ºge Gaze-spezifische Schichten hinzu\n",
    "            self.fc_pitch = nn.Linear(2048, 90)  # 90 Bins f√ºr Pitch\n",
    "            self.fc_yaw = nn.Linear(2048, 90)    # 90 Bins f√ºr Yaw\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.backbone(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            pitch = self.fc_pitch(x)\n",
    "            yaw = self.fc_yaw(x)\n",
    "            return pitch, yaw\n",
    "    \n",
    "    # Modell erstellen und auf Ger√§t verschieben\n",
    "    model = SimpleGazeModel()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Vereinfachtes Gaze-Modell erstellt!\")\n",
    "    \n",
    "    # Transformationen f√ºr Eingabebilder\n",
    "    from torchvision import transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Vereinfachte Gaze Pipeline\n",
    "    class SimpleGazePipeline:\n",
    "        def __init__(self, model, device, transform):\n",
    "            self.model = model\n",
    "            self.device = device\n",
    "            self.transform = transform\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.idx_tensor = torch.FloatTensor([idx for idx in range(90)]).to(device)\n",
    "            \n",
    "            # Versuche Gesichtsdetektor zu laden\n",
    "            try:\n",
    "                from face_detection import RetinaFace\n",
    "                self.detector = RetinaFace()\n",
    "                self.has_detector = True\n",
    "                print(\"Gesichtsdetektor verf√ºgbar\")\n",
    "            except:\n",
    "                print(\"Kein Gesichtsdetektor - arbeite mit ganzen Bildern\")\n",
    "                self.detector = None\n",
    "                self.has_detector = False\n",
    "        \n",
    "        def process_frame(self, frame):\n",
    "            \"\"\"Verarbeite ein Frame und gib Gaze-Richtungen zur√ºck\"\"\"\n",
    "            results = {\n",
    "                'pitch': [],\n",
    "                'yaw': [],\n",
    "                'bboxes': []\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                if self.has_detector and self.detector is not None:\n",
    "                    # Versuche Gesichter zu erkennen\n",
    "                    faces = self.detector(frame)\n",
    "                    \n",
    "                    if faces is not None:\n",
    "                        for box, landmark, score in faces:\n",
    "                            if score < 0.5:  # Mindestvertrauen\n",
    "                                continue\n",
    "                            \n",
    "                            # Extrahiere Gesichts-Region\n",
    "                            x1, y1, x2, y2 = map(int, box)\n",
    "                            x1, y1 = max(0, x1), max(0, y1)\n",
    "                            x2 = min(frame.shape[1], x2)\n",
    "                            y2 = min(frame.shape[0], y2)\n",
    "                            \n",
    "                            face_img = frame[y1:y2, x1:x2]\n",
    "                            if face_img.size == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            # Verarbeite das Gesichtsbild\n",
    "                            pitch, yaw = self._predict_gaze(face_img)\n",
    "                            \n",
    "                            results['pitch'].append(pitch)\n",
    "                            results['yaw'].append(yaw)\n",
    "                            results['bboxes'].append([x1, y1, x2, y2])\n",
    "                    \n",
    "                else:\n",
    "                    # Fallback: Verwende das ganze Bild\n",
    "                    pitch, yaw = self._predict_gaze(frame)\n",
    "                    results['pitch'].append(pitch)\n",
    "                    results['yaw'].append(yaw)\n",
    "                    h, w = frame.shape[:2]\n",
    "                    results['bboxes'].append([0, 0, w, h])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei Frame-Verarbeitung: {e}\")\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        def _predict_gaze(self, img):\n",
    "            \"\"\"Vorhersage f√ºr ein einzelnes Bild\"\"\"\n",
    "            try:\n",
    "                # Bild transformieren\n",
    "                if len(img.shape) == 3:\n",
    "                    img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "                else:\n",
    "                    return 0.0, 0.0\n",
    "                \n",
    "                # Vorhersage\n",
    "                with torch.no_grad():\n",
    "                    pitch_logits, yaw_logits = self.model(img_tensor)\n",
    "                    \n",
    "                    # Softmax anwenden\n",
    "                    pitch_probs = self.softmax(pitch_logits)\n",
    "                    yaw_probs = self.softmax(yaw_logits)\n",
    "                    \n",
    "                    # Kontinuierliche Werte berechnen\n",
    "                    pitch_deg = torch.sum(pitch_probs * self.idx_tensor, 1) * 4 - 180\n",
    "                    yaw_deg = torch.sum(yaw_probs * self.idx_tensor, 1) * 4 - 180\n",
    "                    \n",
    "                    # In Radianten umwandeln\n",
    "                    pitch_rad = pitch_deg * np.pi / 180.0\n",
    "                    yaw_rad = yaw_deg * np.pi / 180.0\n",
    "                    \n",
    "                    return pitch_rad.item(), yaw_rad.item()\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei Gaze-Vorhersage: {e}\")\n",
    "                return 0.0, 0.0\n",
    "    \n",
    "    # Pipeline erstellen\n",
    "    gaze_pipeline = SimpleGazePipeline(model, device, transform)\n",
    "    \n",
    "    print(\"‚úÖ Vereinfachte Gaze Detection Pipeline erfolgreich erstellt!\")\n",
    "    print(\"‚ö†Ô∏è  Hinweis: Dies ist eine Demo-Version. F√ºr Produktionsqualit√§t w√ºrde ein\")\n",
    "    print(\"   speziell trainiertes Gaze-Modell ben√∂tigt werden.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fehler beim Initialisieren: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e11ce",
   "metadata": {},
   "source": [
    "## Schritt 5: Live Webcam-Stream (Interaktiv)\n",
    "\n",
    "F√ºr eine kontinuierliche Gaze Detection √ºber die Webcam. **Hinweis**: Dieser Code funktioniert am besten in einer lokalen Umgebung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9cb9f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Live-Gaze-Detection Funktionen definiert!\n"
     ]
    }
   ],
   "source": [
    "def run_live_gaze_detection(duration_seconds=10):\n",
    "    \"\"\"\n",
    "    F√ºhrt Live-Gaze-Detection f√ºr eine bestimmte Dauer durch.\n",
    "    \n",
    "    Args:\n",
    "        duration_seconds: Dauer der Live-Detection in Sekunden\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Fehler: Kann Webcam nicht √∂ffnen\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üé• Starte Live Gaze Detection f√ºr {duration_seconds} Sekunden...\")\n",
    "    print(\"Dr√ºcke 'q' im OpenCV-Fenster zum Beenden\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Fehler beim Lesen des Frames\")\n",
    "                break\n",
    "            \n",
    "            # FPS berechnen\n",
    "            fps_start = time.time()\n",
    "            \n",
    "            # Gaze Detection durchf√ºhren\n",
    "            results = gaze_pipeline.process_frame(frame)\n",
    "            \n",
    "            # Frame f√ºr Visualisierung kopieren\n",
    "            frame_with_gaze = frame.copy()\n",
    "            \n",
    "            # FPS anzeigen\n",
    "            fps = 1.0 / (time.time() - fps_start) if (time.time() - fps_start) > 0 else 0\n",
    "            cv2.putText(frame_with_gaze, f'FPS: {fps:.1f}', (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Frame Counter\n",
    "            cv2.putText(frame_with_gaze, f'Frame: {frame_count}', (10, 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Gaze Information anzeigen und visualisieren\n",
    "            if len(results['pitch']) > 0:\n",
    "                for i in range(len(results['pitch'])):\n",
    "                    pitch_rad = results['pitch'][i]\n",
    "                    yaw_rad = results['yaw'][i]\n",
    "                    pitch_deg = pitch_rad * 180 / np.pi\n",
    "                    yaw_deg = yaw_rad * 180 / np.pi\n",
    "                    \n",
    "                    # Bounding Box zeichnen\n",
    "                    if i < len(results['bboxes']):\n",
    "                        x1, y1, x2, y2 = results['bboxes'][i]\n",
    "                        cv2.rectangle(frame_with_gaze, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        \n",
    "                        # Gaze-Pfeil zeichnen\n",
    "                        center_x = (x1 + x2) // 2\n",
    "                        center_y = (y1 + y2) // 2\n",
    "                        \n",
    "                        arrow_length = 60\n",
    "                        end_x = int(center_x + arrow_length * np.sin(yaw_rad) * np.cos(pitch_rad))\n",
    "                        end_y = int(center_y - arrow_length * np.sin(pitch_rad))\n",
    "                        \n",
    "                        cv2.arrowedLine(frame_with_gaze, (center_x, center_y), \n",
    "                                      (end_x, end_y), (0, 0, 255), 3, tipLength=0.3)\n",
    "                    \n",
    "                    # Text mit Gaze-Werten\n",
    "                    text = f'Face {i+1}: P:{pitch_deg:.0f}¬∞ Y:{yaw_deg:.0f}¬∞'\n",
    "                    cv2.putText(frame_with_gaze, text, (10, 90 + i*30), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame_with_gaze, 'Kein Gesicht erkannt', (10, 90), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "            # Anweisungen anzeigen\n",
    "            cv2.putText(frame_with_gaze, \"Druecke 'q' zum Beenden\", (10, frame.shape[0] - 20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            \n",
    "            # Frame anzeigen\n",
    "            cv2.imshow('Live Gaze Detection - L2CS-Net Demo', frame_with_gaze)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Beenden mit 'q' oder nach der gew√ºnschten Zeit\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                print(\"‚ùå Beendet durch Benutzer (q gedr√ºckt)\")\n",
    "                break\n",
    "            \n",
    "            if time.time() - start_time > duration_seconds:\n",
    "                print(f\"‚è∞ Zeit abgelaufen ({duration_seconds}s)\")\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"‚ùå Unterbrochen durch Benutzer (Ctrl+C)\")\n",
    "    \n",
    "    finally:\n",
    "        # Aufr√§umen\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Statistiken anzeigen\n",
    "        total_time = time.time() - start_time\n",
    "        avg_fps = frame_count / total_time if total_time > 0 else 0\n",
    "        print(f\"\\\\nüìä Statistiken:\")\n",
    "        print(f\"   Gesamtzeit: {total_time:.1f}s\")\n",
    "        print(f\"   Frames verarbeitet: {frame_count}\")\n",
    "        print(f\"   Durchschnittliche FPS: {avg_fps:.1f}\")\n",
    "\n",
    "# Erweiterte Live Detection mit Datensammlung\n",
    "def run_live_gaze_with_recording(duration_seconds=10, save_data=True):\n",
    "    \"\"\"\n",
    "    Live Gaze Detection mit optionaler Datenaufzeichnung\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Fehler: Kann Webcam nicht √∂ffnen\")\n",
    "        return None\n",
    "    \n",
    "    # Datensammler initialisieren\n",
    "    gaze_data = [] if save_data else None\n",
    "    \n",
    "    print(f\"üé• Starte Live Gaze Detection mit Aufzeichnung f√ºr {duration_seconds} Sekunden...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Gaze Detection\n",
    "            results = gaze_pipeline.process_frame(frame)\n",
    "            \n",
    "            # Daten sammeln\n",
    "            if save_data and len(results['pitch']) > 0:\n",
    "                timestamp = time.time() - start_time\n",
    "                for i in range(len(results['pitch'])):\n",
    "                    data_point = {\n",
    "                        'timestamp': timestamp,\n",
    "                        'frame': frame_count,\n",
    "                        'face_id': i,\n",
    "                        'pitch_deg': results['pitch'][i] * 180 / np.pi,\n",
    "                        'yaw_deg': results['yaw'][i] * 180 / np.pi\n",
    "                    }\n",
    "                    gaze_data.append(data_point)\n",
    "            \n",
    "            # Visualisierung (vereinfacht f√ºr Performance)\n",
    "            frame_display = frame.copy()\n",
    "            cv2.putText(frame_display, f'Recording: {len(gaze_data) if gaze_data else 0} points', \n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Gaze-Pfeile zeichnen\n",
    "            if len(results['pitch']) > 0:\n",
    "                for i in range(len(results['pitch'])):\n",
    "                    if i < len(results['bboxes']):\n",
    "                        x1, y1, x2, y2 = results['bboxes'][i]\n",
    "                        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "                        \n",
    "                        pitch_rad, yaw_rad = results['pitch'][i], results['yaw'][i]\n",
    "                        arrow_length = 50\n",
    "                        end_x = int(center_x + arrow_length * np.sin(yaw_rad))\n",
    "                        end_y = int(center_y - arrow_length * np.sin(pitch_rad))\n",
    "                        \n",
    "                        cv2.arrowedLine(frame_display, (center_x, center_y), \n",
    "                                      (end_x, end_y), (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.imshow('Live Gaze Recording', frame_display)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q') or time.time() - start_time > duration_seconds:\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Unterbrochen\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        if save_data and gaze_data:\n",
    "            print(f\"üìà {len(gaze_data)} Gaze-Datenpunkte gesammelt\")\n",
    "            return gaze_data\n",
    "        \n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Live-Gaze-Detection Funktionen definiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a41cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• Starte Live Gaze Detection f√ºr 20 Sekunden...\n",
      "Dr√ºcke 'q' im OpenCV-Fenster zum Beenden\n",
      "‚è∞ Zeit abgelaufen (20s)\n",
      "‚è∞ Zeit abgelaufen (20s)\n",
      "\\nüìä Statistiken:\n",
      "   Gesamtzeit: 20.7s\n",
      "   Frames verarbeitet: 64\n",
      "   Durchschnittliche FPS: 3.1\n",
      "\\nüìä Statistiken:\n",
      "   Gesamtzeit: 20.7s\n",
      "   Frames verarbeitet: 64\n",
      "   Durchschnittliche FPS: 3.1\n"
     ]
    }
   ],
   "source": [
    "# Starte die Live-Gaze-Detection\n",
    "# Passe die Dauer nach Bedarf an (in Sekunden)\n",
    "run_live_gaze_detection(duration_seconds=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bebb90",
   "metadata": {},
   "source": [
    "## Schritt 6: Gaze-Daten mit JSON-Export (10ms Intervall)\n",
    "\n",
    "Diese Funktion sammelt Gaze-Daten alle 10ms und speichert sie als JSON-Datei mit Timestamp, X- und Y-Koordinaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d3d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON-Export Funktionen definiert!\n",
      "üìã Neue Funktion verf√ºgbar:\n",
      "   - run_gaze_recording_with_json_export(duration, filename)\n",
      "\\nüí° Koordinatensystem:\n",
      "   X-Koordinate: -1 (rechts) bis +1 (links)\n",
      "   Y-Koordinate: -1 (oben) bis +1 (unten)\n",
      "   Sampling Rate: 100Hz (alle 10ms)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import threading\n",
    "from datetime import datetime\n",
    "\n",
    "class GazeDataRecorder:\n",
    "    \"\"\"Klasse f√ºr kontinuierliche Gaze-Datenaufzeichnung mit 10ms Intervallen - Multi-Person Support\"\"\"\n",
    "    \n",
    "    def __init__(self, output_file=\"gaze_data.json\"):\n",
    "        self.output_file = output_file\n",
    "        self.recording = False\n",
    "        self.gaze_data = []\n",
    "        self.current_faces = {}  # Dict f√ºr mehrere Gesichter: {face_id: gaze_info}\n",
    "        self.start_time = None\n",
    "        self.recording_thread = None\n",
    "        \n",
    "    def update_gaze(self, pitch_deg, yaw_deg):\n",
    "        \"\"\"\n",
    "        Legacy-Funktion f√ºr Einzelperson (R√ºckw√§rtskompatibilit√§t)\n",
    "        \"\"\"\n",
    "        x = max(-1, min(1, yaw_deg / 180.0))\n",
    "        y = max(-1, min(1, pitch_deg / 180.0))\n",
    "        self.current_faces = {\"face_0\": {\"x\": x, \"y\": y, \"pitch_deg\": pitch_deg, \"yaw_deg\": yaw_deg}}\n",
    "    \n",
    "    def update_multiple_gazes(self, results):\n",
    "        \"\"\"\n",
    "        Aktualisiert Gaze-Positionen f√ºr mehrere Personen\n",
    "        Args:\n",
    "            results: Dict mit 'pitch', 'yaw', 'bboxes' Listen\n",
    "        \"\"\"\n",
    "        self.current_faces = {}\n",
    "        \n",
    "        if len(results['pitch']) > 0:\n",
    "            for i in range(len(results['pitch'])):\n",
    "                pitch_deg = results['pitch'][i] * 180 / np.pi  # Rad zu Grad\n",
    "                yaw_deg = results['yaw'][i] * 180 / np.pi\n",
    "                \n",
    "                # Normalisierung: -180¬∞ bis 180¬∞ -> -1 bis 1\n",
    "                x = max(-1, min(1, yaw_deg / 180.0))\n",
    "                y = max(-1, min(1, pitch_deg / 180.0))\n",
    "                \n",
    "                face_id = f\"face_{i}\"\n",
    "                self.current_faces[face_id] = {\n",
    "                    \"x\": round(x, 4),\n",
    "                    \"y\": round(y, 4),\n",
    "                    \"pitch_deg\": round(pitch_deg, 2),\n",
    "                    \"yaw_deg\": round(yaw_deg, 2)\n",
    "                }\n",
    "    \n",
    "    def _record_loop(self):\n",
    "        \"\"\"Interner Loop f√ºr 10ms Aufzeichnung - Multi-Person\"\"\"\n",
    "        while self.recording:\n",
    "            timestamp_ms = int((time.time() - self.start_time) * 1000)  # Millisekunden seit Start\n",
    "            \n",
    "            # Multi-Person Datenstruktur\n",
    "            faces_data = []\n",
    "            for face_id, gaze_info in self.current_faces.items():\n",
    "                faces_data.append({\n",
    "                    \"face_id\": face_id,\n",
    "                    \"x\": gaze_info[\"x\"],\n",
    "                    \"y\": gaze_info[\"y\"],\n",
    "                    \"pitch_deg\": gaze_info[\"pitch_deg\"],\n",
    "                    \"yaw_deg\": gaze_info[\"yaw_deg\"]\n",
    "                })\n",
    "            \n",
    "            data_point = {\n",
    "                \"timestamp\": timestamp_ms,\n",
    "                \"faces\": faces_data\n",
    "            }\n",
    "            \n",
    "            self.gaze_data.append(data_point)\n",
    "            \n",
    "            # Warte 10ms (0.01 Sekunden)\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    def start_recording(self):\n",
    "        \"\"\"Startet die Aufzeichnung\"\"\"\n",
    "        if not self.recording:\n",
    "            self.recording = True\n",
    "            self.start_time = time.time()\n",
    "            self.gaze_data = []\n",
    "            \n",
    "            # Starte Recording-Thread\n",
    "            self.recording_thread = threading.Thread(target=self._record_loop)\n",
    "            self.recording_thread.daemon = True\n",
    "            self.recording_thread.start()\n",
    "            \n",
    "            print(f\"üìä Gaze-Aufzeichnung gestartet (10ms Intervall)\")\n",
    "    \n",
    "    def stop_recording(self):\n",
    "        \"\"\"Stoppt die Aufzeichnung\"\"\"\n",
    "        if self.recording:\n",
    "            self.recording = False\n",
    "            \n",
    "            # Warte bis Thread beendet ist\n",
    "            if self.recording_thread:\n",
    "                self.recording_thread.join(timeout=1.0)\n",
    "            \n",
    "            print(f\"‚èπÔ∏è  Aufzeichnung gestoppt. {len(self.gaze_data)} Datenpunkte gesammelt\")\n",
    "    \n",
    "    def save_to_json(self, filename=None):\n",
    "        \"\"\"Speichert die Daten als JSON\"\"\"\n",
    "        if filename is None:\n",
    "            filename = self.output_file\n",
    "        \n",
    "        # Metadaten erstellen\n",
    "        end_time = time.time()\n",
    "        duration_ms = int((end_time - self.start_time) * 1000) if self.start_time else 0\n",
    "        \n",
    "        # Z√§hle eindeutige Faces √ºber alle Samples\n",
    "        all_face_ids = set()\n",
    "        for sample in self.gaze_data:\n",
    "            for face in sample.get(\"faces\", []):\n",
    "                all_face_ids.add(face[\"face_id\"])\n",
    "                \n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"recording_date\": datetime.now().isoformat(),\n",
    "                \"duration_ms\": duration_ms,\n",
    "                \"duration_seconds\": round(duration_ms / 1000.0, 2),\n",
    "                \"total_samples\": len(self.gaze_data),\n",
    "                \"sample_rate_hz\": 100,  # 10ms = 100Hz\n",
    "                \"unique_faces\": len(all_face_ids),\n",
    "                \"face_ids\": sorted(list(all_face_ids)),\n",
    "                \"coordinate_system\": {\n",
    "                    \"x_range\": [-1, 1],\n",
    "                    \"y_range\": [-1, 1],\n",
    "                    \"description\": \"Normalized coordinates: x=yaw/180, y=pitch/180\"\n",
    "                },\n",
    "                \"data_format\": \"Multi-person with faces array per timestamp\"\n",
    "            },\n",
    "            \"gaze_data\": self.gaze_data\n",
    "        }\n",
    "        \n",
    "        # JSON speichern\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Gaze-Daten gespeichert: {filename}\")\n",
    "        print(f\"   üìà {len(self.gaze_data)} Samples √ºber {duration_ms}ms\")\n",
    "        return filename\n",
    "\n",
    "def run_gaze_recording_with_json_export(duration_seconds=10, output_file=\"gaze_recording.json\"):\n",
    "    \"\"\"\n",
    "    F√ºhrt Live Gaze Detection durch und speichert Daten alle 10ms als JSON\n",
    "    \n",
    "    Args:\n",
    "        duration_seconds: Aufnahmedauer in Sekunden\n",
    "        output_file: Name der JSON-Ausgabedatei\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"‚ùå Fehler: Kann Webcam nicht √∂ffnen\")\n",
    "        return None\n",
    "    \n",
    "    # Recorder initialisieren\n",
    "    recorder = GazeDataRecorder(output_file)\n",
    "    \n",
    "    print(f\"üé• Starte Gaze Recording f√ºr {duration_seconds} Sekunden...\")\n",
    "    print(f\"üìä Daten werden alle 10ms gesampelt und als JSON gespeichert\")\n",
    "    print(\"Dr√ºcke 'q' im OpenCV-Fenster zum vorzeitigen Beenden\")\n",
    "    \n",
    "    # Recording starten\n",
    "    recorder.start_recording()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Fehler beim Lesen des Frames\")\n",
    "                break\n",
    "            \n",
    "            # Gaze Detection durchf√ºhren\n",
    "            results = gaze_pipeline.process_frame(frame)\n",
    "            \n",
    "            # Gaze-Daten an Recorder weitergeben (alle erkannten Gesichter)\n",
    "            recorder.update_multiple_gazes(results)\n",
    "            \n",
    "            # Visualisierung\n",
    "            frame_display = frame.copy()\n",
    "            \n",
    "            # Status-Informationen\n",
    "            elapsed_time = time.time() - start_time\n",
    "            cv2.putText(frame_display, f'Recording: {elapsed_time:.1f}s / {duration_seconds}s', \n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame_display, f'Samples: {len(recorder.gaze_data)}', \n",
    "                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Aktuelle Gaze-Koordinaten f√ºr alle Gesichter anzeigen\n",
    "            if len(recorder.current_faces) > 0:\n",
    "                y_offset = 90\n",
    "                for face_id, gaze_info in recorder.current_faces.items():\n",
    "                    x_coord = gaze_info[\"x\"]\n",
    "                    y_coord = gaze_info[\"y\"]\n",
    "                    cv2.putText(frame_display, f'{face_id}: X={x_coord:.3f}, Y={y_coord:.3f}', \n",
    "                               (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "                    y_offset += 25\n",
    "                \n",
    "                # Zeichne Gaze-Visualisierung\n",
    "                for i in range(len(results['pitch'])):\n",
    "                    if i < len(results['bboxes']):\n",
    "                        x1, y1, x2, y2 = results['bboxes'][i]\n",
    "                        \n",
    "                        # Bounding Box\n",
    "                        cv2.rectangle(frame_display, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        \n",
    "                        # Gaze-Pfeil\n",
    "                        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "                        arrow_length = 60\n",
    "                        \n",
    "                        pitch_rad = results['pitch'][i]\n",
    "                        yaw_rad = results['yaw'][i]\n",
    "                        end_x = int(center_x + arrow_length * np.sin(yaw_rad) * np.cos(pitch_rad))\n",
    "                        end_y = int(center_y - arrow_length * np.sin(pitch_rad))\n",
    "                        \n",
    "                        cv2.arrowedLine(frame_display, (center_x, center_y), \n",
    "                                      (end_x, end_y), (0, 0, 255), 3, tipLength=0.3)\n",
    "            else:\n",
    "                cv2.putText(frame_display, 'Kein Gesicht erkannt', (10, 90), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "            # Anweisungen\n",
    "            cv2.putText(frame_display, \"Druecke 'q' zum Beenden\", (10, frame.shape[0] - 20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            \n",
    "            # Frame anzeigen\n",
    "            cv2.imshow('Gaze Recording - JSON Export', frame_display)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Beenden-Bedingungen\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                print(\"‚ùå Beendet durch Benutzer (q gedr√ºckt)\")\n",
    "                break\n",
    "            \n",
    "            if time.time() - start_time >= duration_seconds:\n",
    "                print(f\"‚è∞ Aufzeichnung nach {duration_seconds}s beendet\")\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"‚ùå Unterbrochen durch Benutzer (Ctrl+C)\")\n",
    "    \n",
    "    finally:\n",
    "        # Aufr√§umen\n",
    "        recorder.stop_recording()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # JSON speichern\n",
    "        if len(recorder.gaze_data) > 0:\n",
    "            saved_file = recorder.save_to_json()\n",
    "            \n",
    "            # Statistiken\n",
    "            total_time = time.time() - start_time\n",
    "            avg_fps = frame_count / total_time if total_time > 0 else 0\n",
    "            \n",
    "            print(f\"\\\\nüìä Aufzeichnung abgeschlossen:\")\n",
    "            print(f\"   ‚è±Ô∏è  Gesamtzeit: {total_time:.1f}s\")\n",
    "            print(f\"   üé¨ Frames verarbeitet: {frame_count}\")\n",
    "            print(f\"   üìπ Durchschnittliche FPS: {avg_fps:.1f}\")\n",
    "            print(f\"   üìà JSON-Datei: {saved_file}\")\n",
    "            \n",
    "            return saved_file\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Keine Daten aufgezeichnet\")\n",
    "            return None\n",
    "\n",
    "print(\"‚úÖ JSON-Export Funktionen definiert!\")\n",
    "print(\"üìã Neue Funktion verf√ºgbar:\")\n",
    "print(\"   - run_gaze_recording_with_json_export(duration, filename)\")\n",
    "print(\"\\\\nüí° Koordinatensystem:\")\n",
    "print(\"   X-Koordinate: -1 (rechts) bis +1 (links)\")\n",
    "print(\"   Y-Koordinate: -1 (oben) bis +1 (unten)\")\n",
    "print(\"   Sampling Rate: 100Hz (alle 10ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692829ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• Starte Gaze Recording f√ºr 30 Sekunden...\n",
      "üìä Daten werden alle 10ms gesampelt und als JSON gespeichert\n",
      "Dr√ºcke 'q' im OpenCV-Fenster zum vorzeitigen Beenden\n",
      "üìä Gaze-Aufzeichnung gestartet (10ms Intervall)\n",
      "‚ùå Beendet durch Benutzer (q gedr√ºckt)\n",
      "‚èπÔ∏è  Aufzeichnung gestoppt. 656 Datenpunkte gesammelt\n",
      "üíæ Gaze-Daten gespeichert: multi_person_gaze_aufzeichnung.json\n",
      "   üìà 656 Samples √ºber 11681ms\n",
      "\\nüìä Aufzeichnung abgeschlossen:\n",
      "   ‚è±Ô∏è  Gesamtzeit: 11.8s\n",
      "   üé¨ Frames verarbeitet: 21\n",
      "   üìπ Durchschnittliche FPS: 1.8\n",
      "   üìà JSON-Datei: multi_person_gaze_aufzeichnung.json\n"
     ]
    }
   ],
   "source": [
    "# Multi-Person Aufzeichnung - Aktiviere diese Zeile f√ºr die Aufzeichnung:\n",
    "recorded_file = run_gaze_recording_with_json_export(\n",
    "    duration_seconds=60, \n",
    "    output_file=\"multi_person_gaze_aufzeichnung.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotionsdetektion_elena_ryumina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
